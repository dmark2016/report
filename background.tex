\section{Background}
\label{sec:background}
\subsection{Prefetchers}
%The background section covers knowledge that is necessary
%for understanding the proposed solution, but you cannot expect
%the audience of the paper to be familiar with. This is commonly
%the case when you propose solutions that incorporate
%knowledge from different research areas. An example would
%be an implementation of a compression scheme between
%memory and processor in order to decrease memory bandwidth
%utilization. It is very likely that you would need to explain
%the compression scheme, since most computer architecture
%researchers would not be familiar with it.

% == This stuff can probably be assumed known ==
%The speed of processors have been improving much faster than the
%speed of memory and thus memory is not able to keep up with the
%processors. Processors have to wait for too long if needed data
%must be fetched from memory. Caches that are smaller, but faster
%than main memory are thus used to store data which is in use by
%the processor. As the cache needs to be a lot faster than main
%memory, it has to be small to not be too expensive. Therefore it
%is limited what can be stored in the cache

With a simple cache, data is not fetched to the cache before the
processor needs the data or data in the same block. These misses
are not possible to avoid without fetching blocks before the
processor needs them (prefetching). There are many strategies
available for deciding what to prefetch.

\subsubsection{Sequential prefetching}

The simplest form of prefetching is to just prefetch the next block
as well when fetching a block the processor needs. This can
sometimes be improved on by changing the prefetch distance (instead
of prefetching the next block, prefetch the second or a later one).
Sometimes the processor will need the next block before it is in
the cache even if it was prefetched. In these cases prefetching a
later block can improve performance.

In addition, more than one block can be prefetched (increasing the
prefetch degree). If the running program uses a lot of sequential
memory this can improve performance. If too many blocks are
prefetched, blocks that are still in use can be evicted from the
cache, which can degrade performance.

\subsubsection{Stride Directed Prefetching}

With stride directed prefetching (SDP), the memory address an instruction
accesses is stored together with the program counter address of the instruction
and a valid bit is set. When an instruction is encountered again, the stride
between the address accessed the last time, and the address accessed the
current time is calculated. Then this stride is added to the address currently
being accessed and the block containing the current address is prefetched.

\subsubsection{Reference Prediction Tables}

The reference prediction (RPT) method is similar to SDP, but in
addition stores the stride and a state. The first time an
instruction is encountered, an entry for that address is added to a table
and the entry is set to the initial state. The second time an
instruction is encountered, the stride is calculated and stored as in SDP.
The state changes to training. The next time the instruction is
encountered, the stride is calculated again, and compared to the
currently stored stride. If they are equal, the state changes to
prefetching and the stride added to the current address is
prefetched.

\subsubsection{Global History Buffer}

With a global history buffer (GHB), all the accesses by the same
instructions are stored in a linked list. Each entry points to
the previous entry which came from the same instruction. This
history can be used by a variety of different algorithms to
determine what to prefetch.

\subsubsection{Delta Correlation}

Using delta correlation, the access history is traversed and the
deltas between the addresses accessed are stored. Then the delta
history is searched to see if a pair equal to the most recent pair
of deltas exists. If such a pair is found, the next delta (or
several of the next deltas) is added to the current address,
and the result is prefetched.

\subsection{Performance measures}
Several different characteristics are used to measure the performance
of the prefetchers. The most important one is the speedup of a
prefetcher compared to running without a prefetcher:

$speedup = \frac{execution time_{without prefetcher}}{execution time_{with prefetcher}}$.

In additon, accuracy and coverage are useful measures. Accuracy measures
how many of the prefetches were actually needed:

$accuracy = \frac{good prefetches}{total prefetches}$.

Even if the prefetches were needed it is not guaranteed that they
were used. They may have been issued too late.

Coverage measures how many of the misses were removed by the
prefetcher:

$coverage = \frac{good prefetches}{cache misses without prefetching}$.

\cite{bib:doc}

\subsection{SPEC CPU2000}
CPU2000 was designed to provide a way to measure performance across
a wide range of hardware. The benchmarks are based on real user
applications. They measure the performance of the processor, memory
and the compiler. CPU2000 contains 25 different benchmarks and
measures both integer and floating point performance.

\cite{bib:cpu2000}

\subsection{M5 simulator}
M5 is an open-source hardware simulation system. It supports multiple
different CPU architectures, one of them being Alpha, which we used.
It has an event-driven memory system which among other features supports
flexible arrangements of the components so it is possible to model
multilevel caches. M5 uses a trace-based CPU model to achieve fast and
accurate measuring of the performance of the memory system.

\cite{bib:m5}

\todo{Aggressiveness}

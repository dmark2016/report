\section{Background}
\label{sec:background}
\subsection{Prefetchers}
%The background section covers knowledge that is necessary
%for understanding the proposed solution, but you cannot expect
%the audience of the paper to be familiar with. This is commonly
%the case when you propose solutions that incorporate
%knowledge from different research areas. An example would
%be an implementation of a compression scheme between
%memory and processor in order to decrease memory bandwidth
%utilization. It is very likely that you would need to explain
%the compression scheme, since most computer architecture
%researchers would not be familiar with it.

% == This stuff can probably be assumed known ==
%The speed of processors have been improving much faster than the
%speed of memory and thus memory is not able to keep up with the
%processors. Processors have to wait for too long if needed data
%must be fetched from memory. Caches that are smaller, but faster
%than main memory are thus used to store data which is in use by
%the processor. As the cache needs to be a lot faster than main
%memory, it has to be small to not be too expensive. Therefore it
%is limited what can be stored in the cache

A simple cache only fetches data into the cache when the data is needed by the processor.
Cache misses are not possible to avoid without fetching blocks before the processor needs them, that is, prefetching.
There are several prefetch strategies available.

\subsubsection{Sequential prefetching}

The simplest form of prefetching is to just prefetch the next block
as well when fetching a block the processor needs. This can
sometimes be improved by changing the prefetch distance. With a
different prefetch distance, the second or a later block will be
prefetched instead of the next block.
Sometimes the processor will need the next block before it is in
the cache even if it was prefetched. In these cases prefetching a
later block can improve performance.

In addition, more than one block can be prefetched, that is, increasing the
prefetch degree. If the running program uses a lot of sequential
memory this can improve performance. If too many blocks are
prefetched, blocks that are still in use can be evicted from the
cache, which can degrade performance.

\subsubsection{Stride Directed Prefetching}

With stride directed prefetching (SDP), the memory address and instruction
accesses is stored together with the program counter address of the instruction
and a valid bit is set. When an instruction is encountered again, the stride
between the address accessed the last time, and the address accessed the
current time is calculated. Then the stride is added to the address currently
being accessed and the block containing the current address is prefetched.

\subsubsection{Reference Prediction Tables}

The reference prediction table (RPT) method is similar to SDP, but in
addition stores the stride and a state. The first time an
instruction is encountered, an entry for that address is added to a table
and the entry is set to the initial state. The second time an
instruction is encountered, the stride is calculated and stored as in SDP.
The state changes to training. The next time the instruction is
encountered, the stride is calculated again, and compared to the
currently stored stride. If they are equal, the state changes to
prefetching and the stride added to the current address is
prefetched.

\subsubsection{Global History Buffer}

With a global history buffer (GHB), all accesses by the same
instructions are stored in a linked list. Each entry points to
the previous entry which came from the same instruction. This
history can be used by a variety of different algorithms to
determine what to prefetch.

\subsubsection{Delta Correlation}

Using delta correlation, the access history is traversed and the
deltas between the addresses accessed are stored. Then the delta
history is searched to see if a pair equal to the most recent pair
of deltas exists. If such a pair is found, the next delta, or
several of the next deltas, are added to the current address,
and the result is prefetched.

\subsection{Performance measures}
Several different characteristics are used to measure the performance
of the prefetchers. The most important one is the speedup of a
prefetcher compared to running without a prefetcher:

$speedup = \frac{execution time_{without prefetcher}}{execution time_{with prefetcher}}$.

In additon, accuracy and coverage are useful measures. Accuracy measures
how many of the prefetches were actually needed:

$accuracy = \frac{good prefetches}{total prefetches}$.

Even if the prefetches were needed it is not guaranteed that they
were used. They may have been issued too late.

Coverage measures how many of the misses were removed by the
prefetcher:

$coverage = \frac{good prefetches}{cache misses without prefetching}$.

\cite{bib:doc}
